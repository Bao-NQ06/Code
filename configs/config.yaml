# =============================================================================
# Whisper Vietnamese Fine-tuning — RTX 5090 Configuration
# 32 GB VRAM, 108.1 TFLOPS, BF16, Flash Attention 2
# =============================================================================

# --- Model ---
model:
  name: "openai/whisper-large-v3"
  language: "vi"
  task: "transcribe"
  freeze_encoder: false
  freeze_embed: false
  gradient_checkpointing: false   # 32 GB VRAM: not needed, saves recompute overhead
  flash_attention: true           # Flash Attention 2 — requires flash-attn package
  torch_compile: false            # torch.compile — enable after confirming stability

# --- LoRA ---
lora:
  enabled: true
  r: 64                    # Higher rank: more capacity for accent/dialect nuances
  alpha: 128               # alpha = 2r keeps effective scale = 2.0
  dropout: 0.05
  target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "out_proj"
    - "fc1"
    - "fc2"
  bias: "none"

# --- Dialect-Adaptive Auxiliary Task (DAAT) — Novel research contribution ---
# Adds a lightweight MLP head on pooled encoder output to classify dialect
# (Northern / Central / Southern). Joint loss pushes encoder to be
# dialect-discriminative, improving per-region WER without inference overhead.
dialect_adapter:
  enabled: true
  num_dialects: 3          # Northern=0, Central=1, Southern=2
  hidden_dim: 256
  dropout: 0.1
  aux_loss_weight: 0.1     # λ in: L = L_ASR + λ * L_dialect_CE

# --- MoE (optional, off by default) ---
moe:
  enabled: false
  num_experts: 8
  top_k: 2
  capacity_factor: 1.25
  load_balance_weight: 0.01
  apply_to: "decoder"
  replace_every_n: 2

# --- Data ---
data:
  train_data_dir: "data/train"
  val_split: 0.05
  test_split: 0.05
  seed: 42
  max_audio_length: 30.0
  sample_rate: 16000
  num_workers: 8           # Full parallelism — no RAM constraint
  pin_memory: true         # Faster CPU→GPU transfer via page-locked memory
  prefetch_factor: 4       # Pre-load 4 batches per worker
  cache_files: 8           # Number of parquet files to keep in per-worker RAM
  audio_column: "audio"
  text_column: "text"
  province_column: "province_name"
  gender_column: "gender"

# --- Training ---
training:
  batch_size: 32           # 32 GB VRAM handles large-v3 at batch=32 with BF16
  gradient_accumulation_steps: 1
  max_epochs: 10
  learning_rate: 5.0e-5    # Whisper large-v3 benefits from smaller LR
  weight_decay: 0.01
  warmup_steps: 500
  max_steps: -1
  fp16: false
  bf16: true               # BF16 is more numerically stable than FP16
  gradient_clip_val: 1.0
  val_check_interval: 0.25
  log_every_n_steps: 10
  save_top_k: 3
  early_stopping_patience: 5

# --- Optimizer ---
optimizer:
  name: "adamw"
  scheduler: "cosine"
  num_warmup_steps: 500
  betas: [0.9, 0.98]       # Whisper paper uses β2=0.98
  eps: 1.0e-6

# --- Paths ---
paths:
  output_dir: "outputs"
  checkpoint_dir: "outputs/checkpoints"
  log_dir: "outputs/logs"

# --- Logging ---
logging:
  logger: "wandb"
  project_name: "whisper-vi-finetune"
  run_name: null
  wandb:
    project: "whisper-vi-finetune"
    run_name: null
    tags: ["whisper", "vietnamese", "lora", "vimd", "large-v3", "daat", "rtx5090"]
    notes: "Whisper large-v3 + LoRA (r=64) + Dialect-Adaptive Auxiliary Task"
    group: null
    log_model: false
    log_predictions: true
    log_audio: false
    log_predictions_every_n_steps: 500

# --- Hardware ---
hardware:
  accelerator: "gpu"
  devices: 1
  strategy: "auto"
  precision: "bf16-mixed"
